{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIa7dZc1+LdgFJxyZzgEkk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poseidon-rust2/extract_income_statements/blob/main/extract_income_statements.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install      pinecone-client\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.32.1 --progress-bar off\n",
        "!pip install -qqq datasets==2.14.4 --progress-bar off\n",
        "!pip install -qqq peft==0.5.0 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.299 --progress-bar off\n",
        "!pip install -qqq bitsandbytes==0.41.1 --progress-bar off\n",
        "!pip install -qqq trl==0.7.1 --progress-bar off\n",
        "!pip install -qqq xformers==0.0.21 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n",
        "!pip install -qqq tokenizers==0.14.0 --progress-bar off\n",
        "!pip install -qqq optimum==1.13.1 --progress-bar off\n",
        "!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off\n",
        "!pip install -qqq unstructured==0.10.16 --progress-bar off\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "_JBOqu9VsrAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer\n",
        "\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "bhmPIhp3Lcxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
        "#MODEL_NAME = \"NousResearch/Llama-2-7b-hf\"\n",
        "#MODEL_NAME = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
        "MODEL_NAME = \"TheBloke/Llama-2-7B-GPTQ\""
      ],
      "metadata": {
        "id": "I5ArIkoq6LcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"poseidon-rust2/income_statements_apple\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "aPgSQnzPahsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "42n6VO8RyTGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPTQConfig\n",
        "from peft import prepare_model_for_kbit_training\n",
        "def create_model_and_tokenizer():\n",
        "    quantization_config_loading = GPTQConfig(bits=4, disable_exllama=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                                  MODEL_NAME,\n",
        "                                  use_safetensors=True,\n",
        "                                  quantization_config=quantization_config_loading,\n",
        "                                  trust_remote_code=True,\n",
        "                                  device_map=\"auto\"\n",
        "                              )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "lQfDk8LiypPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = create_model_and_tokenizer()\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "eJOKoYgS2BBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.quantization_config.to_dict()"
      ],
      "metadata": {
        "id": "XKek_dz02Bnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"experiments\""
      ],
      "metadata": {
        "id": "US-hOzhG6_hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir experiments/runs"
      ],
      "metadata": {
        "id": "gzAi_nmnwxuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_alpha = 32\n",
        "lora_dropout = 0.05\n",
        "lora_r = 16\n",
        "config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    #target_modules=[\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"],\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "hPrYJES1NOyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    logging_steps=1,\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    num_train_epochs=13,\n",
        "    evaluation_strategy=\"no\",\n",
        "    eval_steps=0.2,\n",
        "    warmup_ratio=0.05,\n",
        "    save_strategy=\"epoch\",\n",
        "    group_by_length=True,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    report_to=\"tensorboard\",\n",
        "    save_safetensors=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=42,\n",
        ")"
      ],
      "metadata": {
        "id": "9Jz_fFv-7vl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    peft_config=config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=3000,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")"
      ],
      "metadata": {
        "id": "WPeTxJRc8DfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "RYh9LeE7_97S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "ygduYYt9-MXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "wcy9V5h9-y3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model"
      ],
      "metadata": {
        "id": "UOJBh7mKtJjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "trained_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    OUTPUT_DIR,\n",
        "    device_map='cuda',\n",
        "    low_cpu_mem_usage=True,\n",
        ")"
      ],
      "metadata": {
        "id": "_4nKlr6Ovuby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inputs = tokenizer(dataset[\"train\"][0][\"text\"], return_tensors=\"pt\").to(DEVICE)\n",
        "#outputs = trained_model.generate(**inputs, max_new_tokens=3000, temperature=0.001)\n",
        "#print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "ilDBs7gL3Kgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from transformers import GenerationConfig, pipeline\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
      ],
      "metadata": {
        "id": "Gyf0k8uvClQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader, OnlinePDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import pinecone\n",
        "import os\n",
        "\n",
        "!gdown \"https://drive.google.com/uc?id=1qu8vT4BH2UyVrfWyqCcQ6Ujj8Xu4CXhH\"\n",
        "loader = PyPDFLoader(\"/content/temp.pdf\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "_bCCMueqI5uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1400, chunk_overlap=300)"
      ],
      "metadata": {
        "id": "lmxqg7CjMzQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs=text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "Kbii7IaHM7MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', 'f5444e56-58db-42db-afd6-d4bd9b2cb40c')\n",
        "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'asia-southeast1-gcp-free')"
      ],
      "metadata": {
        "id": "SrRZ99AuNByq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "tHkWwfnhNIX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,\n",
        "    environment=PINECONE_API_ENV\n",
        ")\n",
        "index_name = \"langchainpinecone\""
      ],
      "metadata": {
        "id": "7pOMQ0o1NTeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch=Pinecone.from_texts([t.page_content for t in docs], embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "fHCiGyRtNZbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"consolidated statements of operations, operating expenses, operating income\"\n",
        "docs=docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "nJluUr3pNkHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "chain=load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "ejjODAsOODHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "information_to_extract = \"Extract consolidated statements of operations over the years in a tsv formatted text.\"\n",
        "chain.run(input_documents=docs, question=information_to_extract)"
      ],
      "metadata": {
        "id": "gaG5VhCBOMH4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}